{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b0ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "#import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "#from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f78ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sumy in c:\\users\\aasna\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: docopt<0.7,>=0.6.1 in c:\\users\\aasna\\anaconda3\\lib\\site-packages (from sumy) (0.6.2)\n",
      "Requirement already satisfied: breadability>=0.1.20 in c:\\users\\aasna\\anaconda3\\lib\\site-packages (from sumy) (0.1.20)\n",
      "Requirement already satisfied: requests>=2.7.0 in c:\\users\\aasna\\anaconda3\\lib\\site-packages (from sumy) (2.28.1)\n",
      "Requirement already satisfied: pycountry>=18.2.23 in c:\\users\\aasna\\anaconda3\\lib\\site-packages (from sumy) (23.12.11)\n",
      "Requirement already satisfied: nltk>=3.0.2 in c:\\users\\aasna\\anaconda3\\lib\\site-packages (from sumy) (3.7)\n",
      "Requirement already satisfied: chardet in c:\\users\\aasna\\anaconda3\\lib\\site-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
      "Requirement already satisfied: lxml>=2.0 in c:\\users\\aasna\\anaconda3\\lib\\site-packages (from breadability>=0.1.20->sumy) (4.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\aasna\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\aasna\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aasna\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aasna\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (4.64.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\aasna\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aasna\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\aasna\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aasna\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (2022.9.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\aasna\\anaconda3\\lib\\site-packages (from click->nltk>=3.0.2->sumy) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\aasna\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\users\\aasna\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6183217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import re\n",
    "import string\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_and_lemmatize(text, custom_stop_words=None):\n",
    "    if custom_stop_words is None:\n",
    "        custom_stop_words = []\n",
    "    stop_words = set(stopwords.words('english')).union(set(custom_stop_words), ENGLISH_STOP_WORDS)\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    pos_tags = pos_tag(words)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Define custom stop words specific to media reviews\n",
    "custom_stop_words = [\n",
    "    'episode', 'season', 'series', 'show', 'watch', 'view', 'character', 'plot', \n",
    "    'scene', 'actor', 'actress', 'director', 'cinema', 'movie', 'film', \n",
    "    'drama', 'comedy', 'thriller', 'action', 'perform', 'role', 'cast', \n",
    "    'seasons', 'episodes', 'viewers', 'watching', 'stream', 'streaming',\n",
    "    'television', 'tv', 'netflix', 'hbo', 'amazon', 'prime', 'disney', \n",
    "    'spoiler', 'spoilers'\n",
    "]\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\Aasna\\Downloads\\All (2).csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Apply preprocessing and lemmatization to the review column\n",
    "df['Lemmatized_Review'] = df['Review'].apply(lambda x: preprocess_and_lemmatize(x, custom_stop_words))\n",
    "\n",
    "# Categorize 'Rating' into bins\n",
    "bins = [0, 4, 7, 10]\n",
    "labels = ['Low', 'Medium', 'High']\n",
    "df['Rating_Category'] = pd.cut(df['Rating'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# One-hot encode the \"Region\" column\n",
    "one_hot_encoder = OneHotEncoder(sparse=True)\n",
    "#encoded_regions = one_hot_encoder.fit_transform(df[['Region']])\n",
    "\n",
    "# Encode the 'Rating_Category' column as the target\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(df['Rating_Category'])\n",
    "\n",
    "# Best n-gram range\n",
    "n_gram_range = (1, 1)\n",
    "\n",
    "# Vectorization with the best n-gram range\n",
    "tfidf_vectorizer_reviews = TfidfVectorizer(ngram_range=n_gram_range)\n",
    "reviews_vectorized = tfidf_vectorizer_reviews.fit_transform(df['Lemmatized_Review'])\n",
    "\n",
    "tfidf_vectorizer_shows = TfidfVectorizer(ngram_range=n_gram_range)\n",
    "shows_vectorized = tfidf_vectorizer_shows.fit_transform(df['Show'])\n",
    "\n",
    "# Combine vectorized features and one-hot encoded 'Region'\n",
    "X = sp.hstack([reviews_vectorized, shows_vectorized])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e252c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New distribution of classes:\n",
      "0    2801\n",
      "2    2801\n",
      "1    2801\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE only on the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# You can check the new distribution of the classes\n",
    "print(\"New distribution of classes:\")\n",
    "print(pd.Series(y_train_smote).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8292445a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category to Class mapping: {'High': 0, 'Low': 1, 'Medium': 2}\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'encoder' is your LabelEncoder instance used to encode 'Rating_Category'\n",
    "categories_to_classes = {category: i for i, category in enumerate(encoder.classes_)}\n",
    "print(\"Category to Class mapping:\", categories_to_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa72ebd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, multi_class='multinomial')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlr = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "mlr.fit(X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d59bdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes:\n",
      "[2 2 2 0 0 2 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 2 0\n",
      " 2 1 2 0 0 0 0 0 0 1 0 1 2 0 1 0 0 0 0 0 0 2 0 2 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 0 0 2 0 0 2 0 0 0 0 0 2 0 1 0 0 0 0 1 1 0 0 2 0 0 0 2 2 0 0 0 0 1 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 2 0 2 1 1 0 1 0 1 2 0 0 0 2 0 1 0 2 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 2 1 0 0 0 0 2 0 0 1 0 1 1 0 0 0 2 2 0 0 1 0 0 0 1 0\n",
      " 0 0 0 0 0 0 2 1 2 0 0 2 0 0 0 0 1 2 1 0 0 0 0 0 2 0 1 0 2 0 0 0 0 2 0 1 2\n",
      " 0 1 0 2 1 0 0 2 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2 0 2 0 0 1 1 1 0 2\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 2 1 0 0 1 2 0 0 2 2 1 1 0 2 1 2 0 0 0 0\n",
      " 0 0 0 0 1 2 1 0 0 1 1 0 1 0 2 2 0 0 1 0 0 0 0 0 2 0 0 0 0 1 0 1 2 2 0 0 2\n",
      " 1 0 1 0 0 0 0 0 0 2 0 0 0 0 2 0 2 0 0 0 0 0 0 2 2 0 0 0 0 1 2 0 0 0 0 0 0\n",
      " 1 1 2 2 1 2 0 0 0 0 0 0 2 0 0 1 0 0 0 0 2 2 0 0 2 0 0 0 1 1 0 0 0 2 0 2 1\n",
      " 0 2 1 0 0 0 1 0 0 0 2 2 2 0 0 2 0 1 2 0 2 0 0 0 0 0 1 0 2 0 0 0 0 2 0 0 0\n",
      " 0 0 0 0 0 0 2 2 1 0 0 0 2 2 2 2 0 2 0 2 1 2 0 1 0 0 0 0 0 0 2 0 2 0 0 0 0\n",
      " 1 0 2 0 0 0 0 0 0 0 1 0 0 1 2 0 0 1 2 0 2 0 1 0 0 0 1 0 1 1 1 2 0 2 0 1 0\n",
      " 2 0 0 0 0 1 0 0 0 0 0 0 2 0 2 1 1 0 2 2 0 0 0 0 0 1 0 0 0 0 1 2 0 2 0 2 0\n",
      " 2 0 1 2 0 0 0 0 0 0 0 2 2 1 0 1 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 2 0 0 1 0 2 0 0 0 0 0 1 0 2 0 1 0 2 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      " 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 2 0 2 2 1 0 2 0 1 2 0 0 0 1\n",
      " 1 2 0 1 1 0 0 0 0 0 0 2 1 1 2 0 1 2 2 0 0 2 0 0 1 0 0 0 0 0 0 2 1 0 0 0 1\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 2 0 0 0 0\n",
      " 0 2 1 0 0 0 2 0 1 0 0 0 0 1 1 0 0 0 2 2 1 0 0 0 2 1 0 0 2 0 0 0 1 0 0 0 0\n",
      " 1 2 0 1 0 0 0 0 0 0 2 0 0 0 0 1 0 0 2 1 0 0 0 2 2 0 0 1 0 2 2 2 0 0 2 1 1\n",
      " 0 1 0 0 0 2 0 2 0 1 0 1 0 0 2 2 0 2 2 0 2 0 1 1 2 0 2 2 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 2 0 0 0 0 1 0 0 0 1 0 2 2 0 0 0 0 0 0 1 2 0 2 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 2 0 0 0 0 0 2 0 1 2 1 1 2 0 1 0 1 1\n",
      " 0 2 0 2 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 2 1 0 0 0 0 1 0 0 2 0 0 2 0 1 0 0\n",
      " 0 0 0 0 0 2 0 0 1 2 0 2 2 2 2 0 0 0 0 1 0 1 1 1 0 0 1 2 0 0 0 2 0 0 0 2 0\n",
      " 0]\n",
      "\n",
      "Predicted probabilities:\n",
      "[[0.38179159 0.12610888 0.49209953]\n",
      " [0.14029387 0.0537262  0.80597993]\n",
      " [0.06470722 0.11326437 0.82202841]\n",
      " ...\n",
      " [0.15296533 0.09815196 0.74888272]\n",
      " [0.98576751 0.00517763 0.00905486]\n",
      " [0.93405818 0.00932349 0.05661833]]\n",
      "\n",
      "Probabilities of the positive class:\n",
      "[1.26108884e-01 5.37261997e-02 1.13264366e-01 4.99145985e-02\n",
      " 3.44378686e-02 3.20495062e-02 1.09547782e-02 6.33371452e-01\n",
      " 2.98321526e-02 8.45155293e-01 5.94083528e-01 3.89617256e-03\n",
      " 2.02935071e-01 9.59051099e-01 2.32712418e-02 1.11886652e-01\n",
      " 1.08095287e-01 1.22352739e-01 1.53338990e-02 6.98397494e-02\n",
      " 4.49110481e-01 4.83283497e-02 7.48705347e-01 2.03756370e-02\n",
      " 1.28911584e-02 2.63131990e-04 6.47639513e-01 5.28556020e-02\n",
      " 6.91085977e-02 4.59803948e-03 3.45973357e-02 4.35496126e-03\n",
      " 8.37090258e-01 1.36443274e-02 8.72135908e-02 2.79378229e-01\n",
      " 1.01178026e-02 4.36860063e-01 6.40721290e-01 4.05587138e-01\n",
      " 1.58702491e-01 6.87971760e-02 4.36275684e-02 2.32070584e-02\n",
      " 1.24693140e-02 3.15530305e-01 4.39654168e-01 3.40949857e-03\n",
      " 4.96585244e-01 2.15243872e-01 3.53720674e-02 3.84229885e-01\n",
      " 4.68519614e-02 1.84596916e-02 1.84645116e-02 2.69684162e-01\n",
      " 2.51279591e-02 5.24019756e-02 4.55234749e-01 1.13657228e-01\n",
      " 3.31085360e-01 9.50801780e-02 2.50371139e-01 3.79925771e-03\n",
      " 1.35775876e-02 6.49818508e-02 4.58683112e-02 7.47488060e-03\n",
      " 1.29770857e-02 1.55234160e-02 8.52890430e-02 4.06414766e-02\n",
      " 6.07432224e-01 5.21053522e-01 2.85035752e-02 1.86174564e-02\n",
      " 1.09961230e-01 1.27442509e-01 4.12416178e-03 2.11379083e-01\n",
      " 2.61445101e-01 3.15447677e-02 1.14761755e-01 3.35428666e-02\n",
      " 2.43773413e-02 7.78527164e-02 1.01693836e-02 7.80102395e-01\n",
      " 6.08423068e-02 6.78273994e-03 2.26277566e-01 3.17544146e-02\n",
      " 5.96309003e-01 3.93715669e-01 6.13977291e-03 1.90060567e-02\n",
      " 1.28386826e-01 9.61592466e-02 3.04646432e-01 6.21562772e-02\n",
      " 2.49051067e-01 1.90743088e-01 5.42954718e-02 2.64178848e-02\n",
      " 7.51403541e-02 2.91079838e-02 6.90102690e-01 1.38326307e-01\n",
      " 1.38922431e-01 7.63786760e-01 1.84956160e-01 2.78684307e-02\n",
      " 1.16817114e-02 6.55501518e-02 2.37940185e-02 1.37562267e-02\n",
      " 6.33809797e-02 6.42837269e-01 2.39829068e-02 1.76926375e-01\n",
      " 1.94782086e-01 1.21167545e-02 2.66256605e-03 8.35943561e-02\n",
      " 4.43893252e-02 8.77952605e-01 2.94326962e-02 8.86507644e-02\n",
      " 3.52833910e-01 3.46450678e-01 1.35418706e-01 4.70190211e-01\n",
      " 4.48430475e-01 3.30001442e-01 4.24963299e-01 4.07017539e-02\n",
      " 5.64496522e-01 3.04162721e-01 2.34828013e-01 6.03440797e-04\n",
      " 5.19311068e-03 3.46969086e-01 4.19307401e-02 4.06003363e-01\n",
      " 1.07056086e-01 3.92591806e-02 1.08689242e-02 7.85874833e-02\n",
      " 2.36659479e-01 5.51656211e-01 1.04519837e-01 6.26630174e-04\n",
      " 6.10861710e-03 2.50953164e-02 8.83619918e-02 1.02208275e-01\n",
      " 5.42203114e-01 2.79461102e-02 6.01068772e-02 3.04875029e-01\n",
      " 3.75906639e-01 1.54037627e-03 1.24448324e-02 1.26790263e-01\n",
      " 2.90237857e-02 1.19190640e-01 2.28981766e-01 1.04653154e-02\n",
      " 5.16295088e-01 2.17562652e-02 4.07591209e-01 6.94792695e-01\n",
      " 2.08736588e-02 5.00228995e-02 2.51874257e-02 1.41937630e-02\n",
      " 1.92488888e-01 2.76288045e-02 8.03366004e-02 4.23969653e-01\n",
      " 1.52756643e-01 2.83078649e-01 3.70971383e-01 5.04330215e-01\n",
      " 5.78992235e-02 1.49436609e-01 1.50164685e-02 1.71991094e-02\n",
      " 4.34076464e-04 2.73681411e-01 4.19636133e-02 4.06626587e-01\n",
      " 8.40479393e-01 2.32828408e-02 3.25405278e-02 1.43674299e-02\n",
      " 4.08525584e-02 3.94143336e-02 8.79868098e-02 3.01989270e-03\n",
      " 2.84615437e-02 3.64909875e-01 2.56482369e-02 4.06249651e-01\n",
      " 2.32126698e-02 1.96240611e-01 1.46513097e-01 4.94322515e-03\n",
      " 1.44600391e-02 9.42269743e-03 8.09249106e-03 5.90159746e-01\n",
      " 3.24834493e-01 1.62664371e-01 7.64126213e-02 1.78622037e-01\n",
      " 3.45739122e-02 9.71362527e-03 1.21924542e-01 3.57499696e-01\n",
      " 8.89692722e-01 3.00600302e-01 3.46898199e-02 4.92519246e-01\n",
      " 2.87924041e-02 1.55609616e-01 8.60449483e-01 1.30687755e-01\n",
      " 1.15705579e-02 5.41052638e-02 2.14865571e-01 6.12259652e-02\n",
      " 1.02877070e-02 7.39604749e-03 1.87027278e-04 4.50353483e-03\n",
      " 6.21839575e-03 3.77475746e-01 1.43483206e-01 2.30711589e-02\n",
      " 1.65888649e-02 1.73994103e-04 1.79839141e-02 1.14146820e-01\n",
      " 4.09741422e-02 1.42808312e-02 7.66214892e-02 7.17349856e-03\n",
      " 6.72178545e-02 2.24773156e-01 7.61737889e-03 3.15982954e-02\n",
      " 1.17413682e-01 4.01601878e-01 5.18582351e-01 5.17886325e-01\n",
      " 5.21957243e-01 1.38157178e-02 7.02165359e-02 1.42403127e-01\n",
      " 8.06721950e-01 2.06841232e-01 2.79058852e-02 1.71923474e-02\n",
      " 4.18929203e-02 6.01197606e-03 6.01311645e-02 1.17885834e-03\n",
      " 1.16451904e-01 8.79778518e-02 3.23575482e-03 5.08773092e-01\n",
      " 6.12908430e-02 2.52574615e-01 4.79341718e-01 6.94933401e-02\n",
      " 8.51801722e-03 7.46117931e-01 1.17180079e-02 2.77704891e-01\n",
      " 5.24411851e-01 2.84959278e-01 1.66075735e-03 3.25529697e-02\n",
      " 3.64736962e-01 7.30920719e-02 4.35423627e-01 8.11222722e-01\n",
      " 3.22037545e-01 1.26730386e-01 3.91667743e-01 2.66344364e-02\n",
      " 1.36368880e-01 1.44329964e-01 5.00529838e-03 1.56903632e-02\n",
      " 2.85244689e-01 2.39277969e-01 8.63011523e-03 1.67456975e-01\n",
      " 5.19442667e-01 3.24867580e-01 5.73140538e-01 6.51030101e-02\n",
      " 2.02527402e-03 8.18141180e-01 4.66260726e-01 5.90001141e-03\n",
      " 5.94173059e-01 5.04007230e-02 3.42686812e-01 2.87409817e-01\n",
      " 1.81400526e-01 2.53690721e-02 5.82565723e-01 7.37117825e-03\n",
      " 8.92157057e-02 9.59198981e-02 1.11267574e-02 8.30172112e-02\n",
      " 1.59319191e-01 1.13852941e-02 4.39054988e-02 1.80848460e-01\n",
      " 1.56363373e-01 7.74423681e-01 6.51346689e-02 6.59620236e-01\n",
      " 4.61713304e-01 2.10288126e-01 1.21939298e-02 2.51520031e-03\n",
      " 2.57439604e-01 5.74320522e-01 2.23887388e-01 8.28197698e-01\n",
      " 3.97304150e-02 1.05093202e-01 3.16731350e-02 9.59746688e-03\n",
      " 9.45186012e-03 2.41072906e-02 1.92588676e-01 9.43739526e-02\n",
      " 4.85035348e-02 3.13115716e-01 2.95793672e-01 3.10790870e-01\n",
      " 1.49511568e-01 2.64663407e-01 1.20653531e-01 3.92648424e-02\n",
      " 1.34406325e-01 1.82477347e-03 9.24315624e-02 2.16872469e-02\n",
      " 3.16237475e-01 2.28346896e-02 1.09540540e-01 8.35863749e-02\n",
      " 2.04254332e-03 8.32202438e-02 5.70437901e-01 7.63621503e-02\n",
      " 3.13865363e-01 5.92605032e-02 2.26908641e-02 4.50036927e-02\n",
      " 2.73726100e-02 4.96798004e-02 4.57975702e-01 6.66580433e-01\n",
      " 1.82352399e-02 8.80495800e-02 9.09140338e-01 3.35123889e-01\n",
      " 1.25666235e-02 3.77746821e-02 1.14810020e-01 3.44077332e-01\n",
      " 2.08872890e-02 3.49058947e-02 4.46391130e-01 3.28697067e-02\n",
      " 2.19879104e-01 5.12910426e-01 5.27110561e-02 1.20478482e-01\n",
      " 3.45349623e-02 2.66676715e-01 4.90257612e-02 3.93691584e-01\n",
      " 2.36748787e-01 1.57208074e-02 1.94637317e-01 6.57642795e-02\n",
      " 4.42586752e-02 9.53368676e-02 4.76681963e-01 6.73026498e-01\n",
      " 1.37527362e-01 2.05959130e-01 5.34327546e-03 5.73489371e-02\n",
      " 8.54969442e-02 2.14237105e-01 4.98949913e-01 6.22493084e-02\n",
      " 3.07349369e-01 8.95510607e-01 2.53593766e-02 3.79528206e-02\n",
      " 1.91990152e-01 3.56244168e-01 2.48307826e-01 1.19972431e-02\n",
      " 7.35745380e-02 8.00108014e-02 3.36858808e-01 3.29909850e-01\n",
      " 4.14513568e-02 2.75163774e-02 8.43823148e-02 8.49277899e-02\n",
      " 4.31471761e-01 1.14446007e-01 1.74880148e-01 1.78060456e-01\n",
      " 1.24844776e-02 7.44342439e-02 1.42047019e-02 1.16351007e-03\n",
      " 1.85699990e-01 5.61806839e-01 4.36182394e-02 4.42558900e-02\n",
      " 5.04796837e-03 1.06920298e-01 1.51393872e-01 2.73433655e-02\n",
      " 7.67114576e-02 2.48134744e-02 1.73653490e-01 5.47404300e-02\n",
      " 9.37812281e-02 4.82639255e-02 1.16962188e-01 5.00288515e-02\n",
      " 3.22927984e-02 5.29404019e-02 3.95039163e-01 1.11314235e-01\n",
      " 5.45299350e-01 1.88390825e-02 7.57082917e-02 4.43406018e-02\n",
      " 2.33571875e-01 4.04688127e-01 3.00983465e-01 9.52342348e-02\n",
      " 2.66427801e-01 8.13094520e-02 1.68542420e-02 8.72085335e-02\n",
      " 4.13968035e-01 1.61403845e-01 2.42054157e-01 8.68421625e-01\n",
      " 3.04992259e-01 1.64089951e-01 1.45053149e-01 1.52012410e-02\n",
      " 1.72499884e-01 4.44085920e-02 1.55911056e-01 2.56011420e-03\n",
      " 3.29179540e-01 6.38079823e-03 1.75393985e-01 1.17515175e-01\n",
      " 1.16819656e-01 8.00146396e-01 8.73247493e-02 2.95466384e-01\n",
      " 2.93589686e-04 1.03021984e-02 1.19357992e-01 8.48567901e-02\n",
      " 3.83006382e-01 5.70965398e-02 9.00720962e-03 4.96211824e-01\n",
      " 1.02997199e-02 5.36680509e-02 8.52194114e-01 1.57733340e-01\n",
      " 1.26618611e-01 1.54211612e-01 8.10097443e-01 9.09134921e-02\n",
      " 4.33791283e-02 2.47535838e-02 3.64016433e-02 6.06664741e-01\n",
      " 1.05049641e-01 2.02696062e-03 1.31699095e-01 5.16210733e-01\n",
      " 2.77703545e-02 6.28626490e-01 4.14112161e-01 6.66459051e-01\n",
      " 7.47801588e-02 8.23011213e-02 1.39798795e-01 2.80729216e-02\n",
      " 8.80082154e-01 2.86815196e-02 4.13735820e-01 2.09797577e-01\n",
      " 6.46110300e-02 4.76305186e-03 1.50829835e-02 5.10353169e-01\n",
      " 8.82561992e-03 3.24967555e-01 4.55392110e-02 1.22300610e-01\n",
      " 6.00340674e-02 1.23742061e-01 2.03229637e-01 6.77808498e-02\n",
      " 3.69242646e-02 4.63554901e-01 5.66976512e-01 1.46659478e-01\n",
      " 1.49212409e-01 4.14310250e-02 1.18252447e-02 1.17383713e-03\n",
      " 4.19031563e-02 3.21084706e-02 4.33126332e-02 6.05530440e-01\n",
      " 3.77604029e-02 5.55329793e-02 1.51144972e-01 1.22498770e-01\n",
      " 4.74566440e-01 4.26987897e-02 2.67921080e-03 2.23282547e-01\n",
      " 4.87665934e-02 2.76346449e-01 3.42481988e-02 1.28513624e-01\n",
      " 5.82342728e-02 6.48652837e-01 9.96541161e-02 1.70109257e-01\n",
      " 9.14596188e-02 9.32346378e-02 3.12127437e-01 4.25121542e-02\n",
      " 3.44552553e-01 1.12021161e-02 3.90467146e-02 4.49824845e-03\n",
      " 5.95534999e-01 1.56711775e-01 7.88641953e-01 3.90320339e-02\n",
      " 7.47987314e-02 8.36674054e-02 8.50903989e-03 1.62854877e-02\n",
      " 1.55799550e-02 2.13762251e-02 1.33067539e-01 8.74471335e-02\n",
      " 5.69103720e-01 1.55667597e-01 2.67273818e-02 3.57236658e-02\n",
      " 5.80221007e-03 5.50727541e-03 4.63675491e-03 1.04256729e-01\n",
      " 3.21342434e-02 2.87766771e-02 2.05235863e-03 5.52281213e-03\n",
      " 6.53631441e-03 4.71331258e-01 1.79704364e-01 1.05386934e-02\n",
      " 3.42016380e-01 5.55689239e-02 2.50675143e-02 7.25911124e-01\n",
      " 3.55848173e-01 2.58379517e-01 2.24262212e-01 3.46636091e-02\n",
      " 1.68929852e-02 1.81638916e-02 1.32156953e-01 5.88742233e-01\n",
      " 2.08366503e-02 1.59907682e-01 1.41498415e-01 5.78879338e-01\n",
      " 5.19656492e-02 2.57703834e-01 3.92741872e-02 6.93383914e-02\n",
      " 8.94210281e-04 5.66136695e-03 9.93337777e-02 2.05359472e-02\n",
      " 7.35027738e-03 7.86298179e-02 4.56319064e-02 1.26575230e-01\n",
      " 5.46692354e-01 2.25341790e-03 1.42903479e-01 5.84625470e-01\n",
      " 7.53323644e-02 5.27627173e-01 2.62895842e-02 1.52206651e-01\n",
      " 3.27112292e-02 6.64907074e-02 6.64269129e-01 1.27489178e-02\n",
      " 5.14965385e-03 4.15403856e-02 3.44250353e-03 7.97111031e-02\n",
      " 7.71787880e-01 7.77361056e-01 1.74927493e-01 7.05507286e-02\n",
      " 7.24626990e-01 3.43933627e-01 2.51505491e-02 1.70208467e-02\n",
      " 2.83743003e-01 6.06064821e-02 1.88697341e-01 1.24663205e-01\n",
      " 1.00577657e-01 1.53940406e-02 1.03889404e-01 2.08751544e-01\n",
      " 3.81187087e-01 4.50580838e-02 8.79677344e-02 3.98194624e-02\n",
      " 5.47312158e-01 3.31655498e-01 9.44292783e-02 8.85148214e-02\n",
      " 2.36424504e-02 6.78242385e-01 5.47046080e-01 1.25231245e-01\n",
      " 1.60433874e-03 7.62085419e-01 8.58860069e-01 2.72111155e-02\n",
      " 8.20228299e-02 1.00020058e-02 2.98859127e-01 9.03683791e-03\n",
      " 4.21937827e-02 2.92902355e-01 5.55392496e-01 4.60446133e-01\n",
      " 1.34635896e-01 1.87627062e-02 9.86287076e-01 1.36394861e-01\n",
      " 3.55962219e-01 1.02267988e-02 8.43540340e-02 8.03759557e-02\n",
      " 1.98508326e-03 2.71672219e-03 6.51735068e-01 1.92324340e-02\n",
      " 2.04135521e-01 2.42003815e-01 1.45286492e-02 4.57294548e-02\n",
      " 3.83655592e-02 1.50061461e-01 5.93493196e-01 1.93364064e-03\n",
      " 1.91930942e-03 2.26000665e-03 7.52401891e-01 1.95613572e-03\n",
      " 4.90273146e-02 6.93769031e-03 7.04727165e-03 1.89227190e-01\n",
      " 1.64047604e-02 6.64362640e-01 1.58377017e-03 5.60204909e-02\n",
      " 9.96302871e-02 2.06054035e-03 1.77077398e-02 1.49456010e-01\n",
      " 1.51864524e-03 2.24910745e-02 1.03987042e-01 3.31714044e-01\n",
      " 3.02296050e-01 9.68993393e-03 5.05568851e-03 3.60327145e-03\n",
      " 4.63484778e-02 2.44530759e-01 1.58169882e-02 1.07284808e-01\n",
      " 2.04698662e-01 7.15699321e-03 3.23141307e-02 3.77465148e-02\n",
      " 8.00846660e-01 6.69573738e-03 2.02581434e-01 1.69615859e-01\n",
      " 2.46963061e-02 3.34877859e-02 7.33082905e-02 2.93285808e-02\n",
      " 7.98080929e-03 3.31418696e-01 4.21319788e-01 7.59802497e-03\n",
      " 1.74283693e-01 1.59761954e-02 6.45995824e-02 2.36456017e-02\n",
      " 5.02035002e-01 9.42085779e-02 1.57009276e-01 1.02394608e-01\n",
      " 1.96115903e-02 9.15463016e-01 7.84732722e-01 3.71027189e-02\n",
      " 6.44199929e-03 1.09586830e-01 7.56483523e-02 3.15271421e-01\n",
      " 5.04560874e-01 6.97740919e-03 1.14583476e-02 8.92979854e-02\n",
      " 3.66862033e-01 3.49058768e-01 1.31443427e-03 3.17008823e-02\n",
      " 5.30354295e-02 2.43343731e-01 8.63966356e-02 1.75976162e-02\n",
      " 6.50949694e-01 1.45393421e-01 4.96325619e-02 2.47890217e-01\n",
      " 2.27704890e-01 4.60973678e-01 4.75068990e-01 1.13847826e-02\n",
      " 9.25830764e-01 8.22166978e-02 4.35158971e-01 5.69340196e-03\n",
      " 3.07282133e-02 4.37377691e-03 2.91938835e-01 5.51818938e-02\n",
      " 7.35810365e-03 1.18817545e-01 3.98681981e-02 4.88091157e-02\n",
      " 5.57157784e-01 2.56751064e-01 1.77503912e-02 2.67423638e-01\n",
      " 7.53537894e-01 6.80812516e-02 4.64096108e-02 1.84530166e-01\n",
      " 4.00970559e-01 3.06886330e-01 7.91845081e-04 5.33792180e-03\n",
      " 7.90826851e-01 8.06566460e-02 1.23996420e-01 2.16722089e-01\n",
      " 2.23132239e-01 8.93552428e-02 1.36100553e-01 1.13623558e-01\n",
      " 7.17397492e-01 7.12221226e-01 4.15004786e-02 8.47327873e-01\n",
      " 1.68070513e-02 3.24553120e-02 3.96026343e-01 1.01371686e-01\n",
      " 7.98397662e-02 7.41305801e-02 1.08976500e-02 5.11095477e-01\n",
      " 7.11356419e-02 4.43110527e-01 6.74381777e-02 5.50204315e-02\n",
      " 2.35022617e-02 3.38193682e-01 7.12626105e-02 1.94553259e-01\n",
      " 4.33585312e-01 1.94442687e-02 1.06736451e-01 2.21700981e-01\n",
      " 7.08678066e-01 3.99068419e-01 2.52057513e-01 2.38077534e-02\n",
      " 5.69583790e-02 8.68383987e-02 1.99533923e-01 3.81908430e-01\n",
      " 6.18215880e-02 4.77747169e-03 4.79796427e-03 4.89955990e-03\n",
      " 3.87449208e-02 5.54619232e-02 1.46092140e-02 9.03867308e-01\n",
      " 6.43077558e-03 3.13919508e-01 7.38956893e-01 1.19622077e-01\n",
      " 4.11464290e-02 1.35839647e-02 1.48231867e-01 9.86012936e-03\n",
      " 6.42054856e-02 5.87237381e-01 1.68356604e-02 2.19968996e-01\n",
      " 3.29790644e-04 5.91283971e-01 1.10567023e-02 1.24541209e-01\n",
      " 1.41429577e-01 5.40672306e-02 1.13127993e-03 4.16475395e-02\n",
      " 8.01922581e-02 1.25442680e-01 7.77100402e-02 7.48893927e-01\n",
      " 2.33827028e-01 5.77942272e-02 3.43718659e-01 1.02050008e-01\n",
      " 4.72987858e-02 7.87018553e-01 4.39228527e-02 3.02849068e-02\n",
      " 1.92069850e-02 1.29713340e-01 8.42494072e-02 1.50403417e-01\n",
      " 3.03725150e-02 1.79093211e-01 5.86001163e-02 2.27327026e-01\n",
      " 9.98591513e-02 1.63326969e-01 1.58887871e-02 7.00352610e-02\n",
      " 2.83768323e-02 8.92670931e-02 4.36691392e-01 8.20076260e-01\n",
      " 3.48852633e-03 2.21049682e-01 1.19723284e-01 9.12339451e-03\n",
      " 3.83076268e-02 2.82527695e-02 2.75236428e-01 2.57858729e-01\n",
      " 4.07654584e-03 1.94872895e-01 1.54577239e-02 1.88710246e-01\n",
      " 1.79497707e-01 4.07447996e-01 2.01154830e-02 4.86719692e-01\n",
      " 3.39440255e-01 5.19781195e-01 3.87444269e-01 1.91514740e-01\n",
      " 4.41131983e-02 8.88473487e-01 7.56022761e-03 5.08748421e-01\n",
      " 5.35473783e-01 3.31686853e-01 7.08102808e-02 1.12813148e-01\n",
      " 1.20570327e-01 1.53153988e-01 5.55146371e-03 2.31619795e-02\n",
      " 4.83983330e-01 1.95553587e-01 3.23774504e-01 4.11012026e-03\n",
      " 4.80575329e-01 6.82118506e-02 4.95115732e-03 3.85812644e-02\n",
      " 3.07120990e-02 2.62324762e-01 1.16577248e-02 9.09870139e-03\n",
      " 5.79719327e-03 4.28764894e-01 6.30387149e-01 1.31072097e-02\n",
      " 1.16977539e-01 6.63002897e-03 1.87245341e-02 5.02144109e-01\n",
      " 8.24733090e-03 3.47778024e-02 4.51120463e-02 2.99414344e-01\n",
      " 2.09226793e-01 7.36494980e-02 4.55928319e-02 8.51516909e-01\n",
      " 2.88393653e-01 5.81445722e-03 3.04107280e-02 3.59979009e-01\n",
      " 5.78211725e-02 4.72269883e-02 3.47134441e-03 3.30332399e-02\n",
      " 4.18077141e-02 6.84690319e-03 5.91570216e-01 3.60742296e-01\n",
      " 3.58194070e-02 2.66857942e-01 1.32661243e-01 1.16202784e-01\n",
      " 3.78405549e-01 4.72981160e-02 4.04770572e-03 4.59668432e-03\n",
      " 1.04046399e-01 4.34329494e-01 3.49712602e-01 3.44704248e-01\n",
      " 8.30739783e-01 5.54718750e-01 2.14992352e-02 2.82161353e-02\n",
      " 4.89609389e-01 2.34616299e-01 1.48979216e-01 3.25653435e-01\n",
      " 3.13648476e-01 1.90649473e-01 9.22020423e-02 3.81066321e-02\n",
      " 8.33099903e-02 9.81519556e-02 5.17762902e-03 9.32348684e-03]\n"
     ]
    }
   ],
   "source": [
    "# Predict with Logistic Regression\n",
    "logreg_predictions = mlr.predict(X_test)\n",
    "# Predicting classes directly\n",
    "predicted_classes = mlr.predict(X_test)\n",
    "print(\"Predicted classes:\")\n",
    "print(predicted_classes)\n",
    "\n",
    "# Predicting probabilities\n",
    "predicted_probabilities = mlr.predict_proba(X_test)\n",
    "print(\"\\nPredicted probabilities:\")\n",
    "print(predicted_probabilities)\n",
    "\n",
    "# If you're interested in the probabilities of the positive class (e.g., success = 1)\n",
    "print(\"\\nProbabilities of the positive class:\")\n",
    "print(predicted_probabilities[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db509831",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1800\\426056714.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Decoding the predictions back to the original labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpredicted_categories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Evaluating the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Decoding the predictions back to the original labels\n",
    "predicted_categories = encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Evaluating the model\n",
    "print(\"Accuracy on Test Set: \", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=encoder.classes_))\n",
    "\n",
    "# Decoding the actual test set categories\n",
    "actual_categories = encoder.inverse_transform(y_test)\n",
    "\n",
    "# Printing a comparison\n",
    "for actual, predicted in zip(actual_categories, predicted_categories):  # Just the first 10 for brevity\n",
    "    print(f\"Actual: {actual}, Predicted: {predicted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab791ac8",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54298f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Custom text preprocessing transformer\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, stop_words=None):\n",
    "        self.stop_words = set(stopwords.words('english')).union(ENGLISH_STOP_WORDS, stop_words if stop_words else [])\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        processed_texts = []\n",
    "        for text in X:\n",
    "            words = word_tokenize(text.lower())\n",
    "            words = [word for word in words if word.isalpha() and word not in self.stop_words]\n",
    "            pos_tags = pos_tag(words)\n",
    "            lemmatized_words = [self.lemmatizer.lemmatize(word, self.get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "            processed_texts.append(' '.join(lemmatized_words))\n",
    "        return np.array(processed_texts)\n",
    "    \n",
    "    def get_wordnet_pos(self, tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "# Define custom stop words specific to media reviews\n",
    "custom_stop_words = [\n",
    "    'episode', 'season', 'series', 'show', 'watch', 'view', 'character', 'plot', \n",
    "    'scene', 'actor', 'actress', 'director', 'cinema', 'movie', 'film', \n",
    "    'drama', 'comedy', 'thriller', 'action', 'perform', 'role', 'cast', \n",
    "    'seasons', 'episodes', 'viewers', 'watching', 'stream', 'streaming',\n",
    "    'television', 'tv', 'netflix', 'hbo', 'amazon', 'prime', 'disney', \n",
    "    'spoiler', 'spoilers'\n",
    "]\n",
    "\n",
    "# Load the dataset (update the file path as needed)\n",
    "df = pd.read_csv(r\"C:\\Users\\m_sha\\Documents\\Text Analytics\\All with regions.csv\")\n",
    "\n",
    "# Preprocessing steps for categorizing 'Rating' into bins and encoding 'Region'\n",
    "df['Rating_Category'] = pd.cut(df['Rating'], bins=[0, 4, 7, 10], labels=['Low', 'Medium', 'High'], include_lowest=True)\n",
    "\n",
    "# Setup for preprocessing and model pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text_review', TfidfVectorizer(preprocessor=lambda x: x, tokenizer=word_tokenize, stop_words=custom_stop_words, ngram_range=(1, 1)), 'Review'),\n",
    "        ('region', OneHotEncoder(), ['Region'])\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Encode the target variable\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(df['Rating_Category'])\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['Review', 'Region']], y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "predicted_categories = encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Output predicted categories\n",
    "print(\"Predicted Categories:\")\n",
    "for i, pred in enumerate(predicted_categories[:10]):  # Example: Output first 10 predictions\n",
    "    print(f\"Sample {i+1}: Predicted Category - {pred}\")\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"\\nAccuracy on Test Set:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb79972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted_categories back to DataFrame for easy filtering\n",
    "predicted_df = pd.DataFrame(predicted_categories, columns=['Predicted_Rating'], index=X_test.index)\n",
    "\n",
    "# Add predicted ratings to the original DataFrame\n",
    "df_with_predictions = df.join(predicted_df, how='inner')\n",
    "\n",
    "# Filter reviews based on predicted rating\n",
    "high_rating_reviews = df_with_predictions[df_with_predictions['Predicted_Rating'] == 'High']['Review']\n",
    "\n",
    "# Assuming 'Show' is the column name for show names and it's included in df_with_predictions\n",
    "# Filter for high-rating reviews and group by 'Show'\n",
    "grouped_reviews = df_with_predictions[df_with_predictions['Predicted_Rating'] == 'High'].groupby('Show')['Review'].apply(' '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886d1fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_predictions_to_reviews(df, predicted_categories, X_test):\n",
    "    # Convert predicted_categories back to DataFrame for easy filtering\n",
    "    predicted_df = pd.DataFrame(predicted_categories, columns=['Predicted_Rating'], index=X_test.index)\n",
    "    \n",
    "    # Add predicted ratings to the original DataFrame\n",
    "    df_with_predictions = df.join(predicted_df, how='inner')\n",
    "    \n",
    "    # Filter for high-rating reviews and group by 'Show'\n",
    "    grouped_reviews = df_with_predictions[df_with_predictions['Predicted_Rating'] == 'High'] \\\n",
    "                                            .groupby('Show')['Review'].apply(' '.join).reset_index()\n",
    "    \n",
    "    return grouped_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36c013",
   "metadata": {},
   "source": [
    "## Original summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dcffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "\n",
    "def simple_summarize(text, num_sentences=3):\n",
    "    sentences = [sent for sent in nltk.sent_tokenize(text)]\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(sentences)\n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    avg = np.mean(cosine_sim, axis=1)\n",
    "    top_idx = np.argsort(avg)[-num_sentences:]\n",
    "    return ' '.join([sentences[idx] for idx in sorted(top_idx)])\n",
    "\n",
    "# Assuming 'Show' column exists and 'df_with_predictions' includes 'Show', 'Review', 'Predicted_Rating'\n",
    "# Filter for high-rating reviews and group by 'Show'\n",
    "grouped_reviews = df_with_predictions[df_with_predictions['Predicted_Rating'] == 'High'].groupby('Show')['Review'].apply(' '.join).reset_index()\n",
    "\n",
    "# Summarize reviews for each show\n",
    "grouped_reviews['Summary'] = grouped_reviews['Review'].apply(lambda text: simple_summarize(text, num_sentences=5))\n",
    "\n",
    "# Print summaries for each show\n",
    "for index, row in grouped_reviews.iterrows():\n",
    "    print(f\"Show: {row['Show']}\")\n",
    "    print(f\"Summary: {row['Summary']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebf2d55",
   "metadata": {},
   "source": [
    "## Summaries by Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950bca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import numpy as np\n",
    "\n",
    "#nltk.download('punkt')  # Make sure to download necessary NLTK data\n",
    "\n",
    "def simple_summarize(text, num_sentences=3):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    num_sentences = min(num_sentences, len(sentences))\n",
    "    if len(sentences) <= num_sentences:\n",
    "        return ' '.join(sentences)\n",
    "    \n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(sentences)\n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    avg = np.mean(cosine_sim, axis=1)\n",
    "    top_idx = np.argsort(avg)[-num_sentences:]\n",
    "    \n",
    "    # Now select the sentences and use sumy for further summarization if needed\n",
    "    selected_sentences = ' '.join([sentences[idx] for idx in sorted(top_idx)])\n",
    "    parser = PlaintextParser.from_string(selected_sentences, Tokenizer(\"english\"))\n",
    "    summarizer = TextRankSummarizer()\n",
    "    summary = summarizer(parser.document, sentences_count=num_sentences)\n",
    "    summary_text = ' '.join([str(sentence) for sentence in summary])\n",
    "    return summary_text\n",
    "\n",
    "def create_role_based_summary(text, role_focus, num_sentences=3):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    keywords = {\n",
    "        'content': ['story', 'character', 'plot', 'engagement'],\n",
    "        'marketing': ['dub', 'sub', 'media', 'hype', 'viewer'],\n",
    "        'studio_production': ['production', 'performance', 'direction', 'budget']\n",
    "    }[role_focus]\n",
    "    \n",
    "    # Filter sentences based on keywords relevant to the role\n",
    "    filtered_sentences = [sentence for sentence in sentences if any(keyword.lower() in sentence.lower() for keyword in keywords)]\n",
    "    \n",
    "    # If not enough sentences after filtering, return the joined filtered sentences\n",
    "    if len(filtered_sentences) <= num_sentences:\n",
    "        return ' '.join(filtered_sentences)\n",
    "    \n",
    "    # Apply TF-IDF and cosine similarity on filtered sentences\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(filtered_sentences)\n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    avg = np.mean(cosine_sim, axis=1)\n",
    "    top_idx = np.argsort(avg)[-num_sentences:]\n",
    "    \n",
    "    # Select the most significant sentences based on cosine similarity\n",
    "    selected_sentences = ' '.join([filtered_sentences[idx] for idx in sorted(top_idx)])\n",
    "    \n",
    "    # Further summarize using sumy's TextRankSummarizer\n",
    "    parser = PlaintextParser.from_string(selected_sentences, Tokenizer(\"english\"))\n",
    "    summarizer = TextRankSummarizer()\n",
    "    summary = summarizer(parser.document, sentences_count=num_sentences)\n",
    "    summary_text = ' '.join([str(sentence) for sentence in summary])\n",
    "    \n",
    "    return summary_text\n",
    "\n",
    "# Example usage\n",
    "grouped_reviews = df_with_predictions[df_with_predictions['Predicted_Rating'] == 'High'].groupby('Show')['Review'].apply(' '.join).reset_index()\n",
    "role_focus = 'content'  # Adjust as needed\n",
    "num_sentences = 2  # Adjust for shorter summaries\n",
    "\n",
    "grouped_reviews['General_Summary'] = grouped_reviews['Review'].apply(lambda text: simple_summarize(text, num_sentences=num_sentences))\n",
    "grouped_reviews[f'{role_focus.capitalize()}_Summary'] = grouped_reviews['Review'].apply(lambda text: create_role_based_summary(text, role_focus, num_sentences=num_sentences))\n",
    "\n",
    "# Print summaries for each show\n",
    "for index, row in grouped_reviews.iterrows():\n",
    "    print(f\"Show: {row['Show']}\")\n",
    "    print(f\"General Summary: {row['General_Summary']}\")\n",
    "    print(f\"{role_focus.capitalize()} Summary: {row[f'{role_focus.capitalize()}_Summary']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e004b",
   "metadata": {},
   "source": [
    "# Final Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf132ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Functions for summarization\n",
    "def filter_positive_sentences(sentences):\n",
    "    \"\"\"Returns only sentences with a positive sentiment.\"\"\"\n",
    "    positive_sentences = []\n",
    "    for sentence in sentences:\n",
    "        blob = TextBlob(sentence)\n",
    "        if blob.sentiment.polarity > 0.2:  # Positive sentiment\n",
    "            positive_sentences.append(sentence)\n",
    "    return positive_sentences\n",
    "\n",
    "def summarize_with_lsa(positive_sentences, num_sentences=3):\n",
    "    \"\"\"Summarizes the text using LSA on filtered positive sentences.\"\"\"\n",
    "    if len(positive_sentences) <= num_sentences:\n",
    "        return ' '.join(positive_sentences)\n",
    "\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(positive_sentences)\n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    avg = np.mean(cosine_sim, axis=1)\n",
    "    top_idx = np.argsort(avg)[-num_sentences:]\n",
    "    \n",
    "    selected_sentences = ' '.join([positive_sentences[idx] for idx in sorted(top_idx)])\n",
    "    parser = PlaintextParser.from_string(selected_sentences, Tokenizer(\"english\"))\n",
    "    summarizer = LsaSummarizer()\n",
    "    summary = summarizer(parser.document, sentences_count=num_sentences)\n",
    "    summary_text = ' '.join([str(sentence) for sentence in summary])\n",
    "    return summary_text\n",
    "\n",
    "def capitalize_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    capitalized_sentences = [sentence[0].upper() + sentence[1:] for sentence in sentences]\n",
    "    return ' '.join(capitalized_sentences)\n",
    "\n",
    "def clean_and_format_summary(text):\n",
    "    # Capitalize the first letter of each sentence\n",
    "    text = capitalize_sentences(text)\n",
    "    \n",
    "    # Fix spacing around punctuation\n",
    "    text = re.sub(r'\\s+([.,;?!])', r'\\1', text)\n",
    "    \n",
    "    # Remove ellipses\n",
    "    text = text.replace('...', '')\n",
    "    \n",
    "    # Remove any redundant whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Ensure proper sentence endings - add a period if missing at the end\n",
    "    if not text.endswith('.'):\n",
    "        text += '.'\n",
    "    \n",
    "    return text\n",
    "\n",
    "def simple_summarize(text, num_sentences=3):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    positive_sentences = filter_positive_sentences(sentences)\n",
    "    summary_text = summarize_with_lsa(positive_sentences, num_sentences)\n",
    "    \n",
    "    # Clean and format summary for professionalism\n",
    "    summary_text_formatted = clean_and_format_summary(summary_text)\n",
    "    \n",
    "    return summary_text_formatted\n",
    "\n",
    "def create_role_based_summary(text, role_focus, num_sentences=3):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    positive_sentences = filter_positive_sentences(sentences)\n",
    "    \n",
    "    keywords = {\n",
    "        'content': ['story', 'character', 'plot', 'engagement'],\n",
    "        'marketing': ['dub', 'sub', 'media', 'hype', 'viewer'],\n",
    "        'studio_production': ['production', 'performance', 'direction', 'budget']\n",
    "    }[role_focus]\n",
    "    \n",
    "    role_filtered_sentences = [sentence for sentence in positive_sentences if any(keyword.lower() in sentence.lower() for keyword in keywords)]\n",
    "    summary_text = summarize_with_lsa(role_filtered_sentences, num_sentences)\n",
    "    \n",
    "    # Clean and format summary for professionalism\n",
    "    summary_text_formatted = clean_and_format_summary(summary_text)\n",
    "    \n",
    "    return summary_text_formatted\n",
    "\n",
    "# Assuming df_with_predictions is defined correctly\n",
    "# Example usage\n",
    "grouped_reviews = df_with_predictions[df_with_predictions['Predicted_Rating'] == 'High'].groupby('Show')['Review'].apply(' '.join).reset_index()\n",
    "role_focus = 'studio_production'  # Adjust as needed\n",
    "num_sentences = 3  # Adjust for shorter summaries\n",
    "\n",
    "grouped_reviews['General_Summary'] = grouped_reviews['Review'].apply(lambda text: simple_summarize(text, num_sentences=num_sentences))\n",
    "grouped_reviews[f'{role_focus.capitalize()}_Summary'] = grouped_reviews['Review'].apply(lambda text: create_role_based_summary(text, role_focus, num_sentences=num_sentences))\n",
    "\n",
    "# Print summaries for each show\n",
    "for index, row in grouped_reviews.iterrows():\n",
    "    print(f\"Show: {row['Show']}\")\n",
    "    print(f\"General Summary: {row['General_Summary']}\")\n",
    "    print(f\"{role_focus.capitalize()} Summary: {row[f'{role_focus.capitalize()}_Summary']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e2e24a",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d33af20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\m_sha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\m_sha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\m_sha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\m_sha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Categories:\n",
      "Sample 1: Predicted Category - High\n",
      "Sample 2: Predicted Category - High\n",
      "Sample 3: Predicted Category - High\n",
      "Sample 4: Predicted Category - Medium\n",
      "Sample 5: Predicted Category - High\n",
      "Sample 6: Predicted Category - High\n",
      "Sample 7: Predicted Category - High\n",
      "Sample 8: Predicted Category - Medium\n",
      "Sample 9: Predicted Category - Medium\n",
      "Sample 10: Predicted Category - High\n",
      "\n",
      "Accuracy on Test Set: 0.766\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        High       0.79      0.98      0.88       688\n",
      "         Low       0.78      0.28      0.41       153\n",
      "      Medium       0.51      0.31      0.38       159\n",
      "\n",
      "    accuracy                           0.77      1000\n",
      "   macro avg       0.69      0.52      0.56      1000\n",
      "weighted avg       0.75      0.77      0.73      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Custom text preprocessing transformer\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, stop_words=None):\n",
    "        self.stop_words = set(stopwords.words('english')).union(ENGLISH_STOP_WORDS, stop_words if stop_words else [])\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        processed_texts = []\n",
    "        for text in X:\n",
    "            words = word_tokenize(text.lower())\n",
    "            words = [word for word in words if word.isalpha() and word not in self.stop_words]\n",
    "            pos_tags = pos_tag(words)\n",
    "            lemmatized_words = [self.lemmatizer.lemmatize(word, self.get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "            processed_texts.append(' '.join(lemmatized_words))\n",
    "        return np.array(processed_texts)\n",
    "    \n",
    "    def get_wordnet_pos(self, tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "# Define custom stop words specific to media reviews\n",
    "custom_stop_words = [\n",
    "    'episode', 'season', 'series', 'show', 'watch', 'view', 'character', 'plot', \n",
    "    'scene', 'actor', 'actress', 'director', 'cinema', 'movie', 'film', \n",
    "    'drama', 'comedy', 'thriller', 'action', 'perform', 'role', 'cast', \n",
    "    'seasons', 'episodes', 'viewers', 'watching', 'stream', 'streaming',\n",
    "    'television', 'tv', 'netflix', 'hbo', 'amazon', 'prime', 'disney', \n",
    "    'spoiler', 'spoilers'\n",
    "]\n",
    "\n",
    "# Load the dataset (update the file path as needed)\n",
    "df = pd.read_csv(r\"C:\\Users\\m_sha\\Documents\\Text Analytics\\All with regions.csv\")\n",
    "\n",
    "# Preprocessing steps for categorizing 'Rating' into bins and encoding 'Region'\n",
    "df['Rating_Category'] = pd.cut(df['Rating'], bins=[0, 4, 7, 10], labels=['Low', 'Medium', 'High'], include_lowest=True)\n",
    "\n",
    "# Setup for preprocessing and model pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text_review', TfidfVectorizer(preprocessor=lambda x: x, tokenizer=word_tokenize, stop_words=custom_stop_words, ngram_range=(1, 1)), 'Review'),\n",
    "        ('region', OneHotEncoder(), ['Region'])\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Encode the target variable\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(df['Rating_Category'])\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['Review', 'Region']], y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "predicted_categories = encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Output predicted categories\n",
    "print(\"Predicted Categories:\")\n",
    "for i, pred in enumerate(predicted_categories[:10]):  # Example: Output first 10 predictions\n",
    "    print(f\"Sample {i+1}: Predicted Category - {pred}\")\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"\\nAccuracy on Test Set:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d8a4812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "class ReviewAnalysisPipeline:\n",
    "    def __init__(self, pipeline, encoder, role_focus='studio_production', num_sentences=3):\n",
    "        self.pipeline = pipeline\n",
    "        self.encoder = encoder\n",
    "        self.role_focus = role_focus\n",
    "        self.num_sentences = num_sentences\n",
    "        self.predicted_df = None \n",
    "    \n",
    "    def process_reviews(self, df, X_test):\n",
    "        # Existing logic to predict and create df_with_predictions\n",
    "        predicted_categories = self.pipeline.predict(X_test)\n",
    "        predicted_categories = self.encoder.inverse_transform(predicted_categories) \n",
    "        self.predicted_df = pd.DataFrame(predicted_categories, columns=['Predicted_Rating'], index=X_test.index)\n",
    "        df_with_predictions = df.join(predicted_df, how='inner')\n",
    "\n",
    "        # Existing logic to filter and group reviews\n",
    "        grouped_reviews = df_with_predictions[df_with_predictions['Predicted_Rating'] == 'High'] \\\n",
    "                          .groupby('Show')['Review'].apply(' '.join).reset_index()\n",
    "\n",
    "        # Return both the grouped_reviews and df_with_predictions\n",
    "        return grouped_reviews, df_with_predictions\n",
    "\n",
    "    # Summarization functions integrated within the class\n",
    "    def filter_positive_sentences(self, grouped_reviews):\n",
    "        positive_sentences = []\n",
    "        for sentence in sentences:\n",
    "            blob = TextBlob(sentence)\n",
    "            if blob.sentiment.polarity > 0.2:\n",
    "                positive_sentences.append(sentence)\n",
    "        return positive_sentences\n",
    "\n",
    "    def summarize_with_lsa(self, positive_sentences):\n",
    "        if len(positive_sentences) <= self.num_sentences:\n",
    "            return ' '.join(positive_sentences)\n",
    "\n",
    "        tfidf = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = tfidf.fit_transform(positive_sentences)\n",
    "        cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "        avg = np.mean(cosine_sim, axis=1)\n",
    "        top_idx = np.argsort(avg)[-self.num_sentences:]\n",
    "        \n",
    "        selected_sentences = ' '.join([positive_sentences[idx] for idx in sorted(top_idx)])\n",
    "        parser = PlaintextParser.from_string(selected_sentences, Tokenizer(\"english\"))\n",
    "        summarizer = LsaSummarizer()\n",
    "        summary = summarizer(parser.document, sentences_count=self.num_sentences)\n",
    "        return ' '.join([str(sentence) for sentence in summary])\n",
    "    \n",
    "    def clean_and_format_summary(self, text, num_sentences):\n",
    "        text = self.capitalize_sentences(text)\n",
    "        text = re.sub(r'\\s+([.,;?!])', r'\\1', text)\n",
    "        text = text.replace('...', '')\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        if not text.endswith('.'):\n",
    "            text += '.'\n",
    "        return text\n",
    "\n",
    "    def capitalize_sentences(self, text, num_sentences):\n",
    "        sentences = sent_tokenize(text)\n",
    "        return ' '.join([sentence[0].upper() + sentence[1:] for sentence in sentences])\n",
    "\n",
    "    def simple_summarize(self, text, num_sentences):\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        positive_sentences = self.filter_positive_sentences(sentences)\n",
    "        summary_text = self.summarize_with_lsa(positive_sentences)\n",
    "        return self.clean_and_format_summary(summary_text)\n",
    "\n",
    "    def create_role_based_summary(self, text, role_focus, num_sentences):\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        positive_sentences = self.filter_positive_sentences(sentences)\n",
    "        keywords = {\n",
    "            'content': ['story', 'character', 'plot', 'engagement'],\n",
    "            'marketing': ['dub', 'sub', 'media', 'hype', 'viewer'],\n",
    "            'studio_production': ['production', 'performance', 'direction', 'budget']\n",
    "        }[role_focus]\n",
    "        \n",
    "        role_filtered_sentences = [sentence for sentence in positive_sentences if any(keyword.lower() in sentence.lower() for keyword in keywords)]\n",
    "        summary_text = self.summarize_with_lsa(role_filtered_sentences)\n",
    "        return self.clean_and_format_summary(summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "600bbbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewAnalysisPipeline:\n",
    "    def __init__(self, pipeline, encoder, role_focus='studio_production', num_sentences=3):\n",
    "        self.pipeline = pipeline\n",
    "        self.encoder = encoder\n",
    "        self.role_focus = role_focus\n",
    "        self.num_sentences = num_sentences\n",
    "        self.predicted_df = None\n",
    "    \n",
    "    def process_reviews(self, df, X_test):\n",
    "        predicted_categories = self.pipeline.predict(X_test)\n",
    "        predicted_categories = self.encoder.inverse_transform(predicted_categories) \n",
    "        self.predicted_df = pd.DataFrame(predicted_categories, columns=['Predicted_Rating'], index=X_test.index)\n",
    "        df_with_predictions = df.join(self.predicted_df, how='inner')  # Use self.predicted_df\n",
    "\n",
    "        grouped_reviews = df_with_predictions[df_with_predictions['Predicted_Rating'] == 'High'] \\\n",
    "                          .groupby('Show')['Review'].apply(' '.join).reset_index()\n",
    "\n",
    "        return grouped_reviews, df_with_predictions\n",
    "    \n",
    "    def summarize_reviews(self, grouped_reviews):\n",
    "        # Apply general summarization\n",
    "        grouped_reviews['General_Summary'] = grouped_reviews['Review'].apply(\n",
    "            lambda text: self.simple_summarize(text))\n",
    "\n",
    "        # Apply role-focused summarization\n",
    "        grouped_reviews[f'{self.role_focus.capitalize()}_Summary'] = grouped_reviews['Review'].apply(\n",
    "            lambda text: self.create_role_based_summary(text, self.role_focus))\n",
    "\n",
    "        return grouped_reviews\n",
    "\n",
    "    def filter_positive_sentences(self, sentences):  # Accepts sentences directly\n",
    "        positive_sentences = []\n",
    "        for sentence in sentences:\n",
    "            blob = TextBlob(sentence)\n",
    "            if blob.sentiment.polarity > 0.2:\n",
    "                positive_sentences.append(sentence)\n",
    "        return positive_sentences\n",
    "\n",
    "    def summarize_with_lsa(self, positive_sentences):\n",
    "        if len(positive_sentences) <= self.num_sentences:\n",
    "            return ' '.join(positive_sentences)\n",
    "\n",
    "        tfidf = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = tfidf.fit_transform(positive_sentences)\n",
    "        cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "        avg = np.mean(cosine_sim, axis=1)\n",
    "        top_idx = np.argsort(avg)[-self.num_sentences:]\n",
    "        \n",
    "        selected_sentences = ' '.join([positive_sentences[idx] for idx in sorted(top_idx)])\n",
    "        parser = PlaintextParser.from_string(selected_sentences, Tokenizer(\"english\"))\n",
    "        summarizer = LsaSummarizer()\n",
    "        summary = summarizer(parser.document, sentences_count=self.num_sentences)\n",
    "        return ' '.join([str(sentence) for sentence in summary])\n",
    "\n",
    "    \n",
    "    def clean_and_format_summary(self, text):\n",
    "        text = self.capitalize_sentences(text)\n",
    "        text = re.sub(r'\\s+([.,;?!])', r'\\1', text)\n",
    "        text = text.replace('...', '')\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        if not text.endswith('.'):\n",
    "            text += '.'\n",
    "        return text\n",
    "\n",
    "    def capitalize_sentences(self, text):\n",
    "        sentences = sent_tokenize(text)\n",
    "        return ' '.join([sentence[0].upper() + sentence[1:] for sentence in sentences])\n",
    "\n",
    "    def get_keywords(self, role_focus):\n",
    "        return {\n",
    "            'content': ['story', 'character', 'plot', 'engagement'],\n",
    "            'marketing': ['dub', 'sub', 'media', 'hype', 'viewer'],\n",
    "            'studio_production': ['production', 'performance', 'direction', 'budget']\n",
    "        }.get(role_focus, [])\n",
    "    \n",
    "    def simple_summarize(self, text):\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        positive_sentences = self.filter_positive_sentences(sentences)\n",
    "        summary_text = self.summarize_with_lsa(positive_sentences)\n",
    "        return self.clean_and_format_summary(summary_text)\n",
    "\n",
    "    def create_role_based_summary(self, text, role_focus):\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        positive_sentences = self.filter_positive_sentences(sentences)\n",
    "        keywords = self.get_keywords(role_focus)\n",
    "        \n",
    "        role_filtered_sentences = [sentence for sentence in positive_sentences if any(keyword.lower() in sentence.lower() for keyword in keywords)]\n",
    "        summary_text = self.summarize_with_lsa(role_filtered_sentences)\n",
    "        return self.clean_and_format_summary(summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a85ba9b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m review_analysis_pipeline \u001b[38;5;241m=\u001b[39m ReviewAnalysisPipeline(pipeline, encoder, role_focus\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudio_production\u001b[39m\u001b[38;5;124m'\u001b[39m, num_sentences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Processing reviews to filter and group high-rating ones\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m grouped_reviews \u001b[38;5;241m=\u001b[39m \u001b[43mreview_analysis_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Summarizing the grouped reviews\u001b[39;00m\n\u001b[0;32m      8\u001b[0m final_summaries \u001b[38;5;241m=\u001b[39m review_analysis_pipeline\u001b[38;5;241m.\u001b[39msummarize_reviews(grouped_reviews)\n",
      "Input \u001b[1;32mIn [63]\u001b[0m, in \u001b[0;36mReviewAnalysisPipeline.process_reviews\u001b[1;34m(self, df, X_test)\u001b[0m\n\u001b[0;32m     24\u001b[0m predicted_categories \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39minverse_transform(predicted_categories) \n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicted_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(predicted_categories, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Rating\u001b[39m\u001b[38;5;124m'\u001b[39m], index\u001b[38;5;241m=\u001b[39mX_test\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m---> 26\u001b[0m df_with_predictions \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mpredicted_df\u001b[49m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Existing logic to filter and group reviews\u001b[39;00m\n\u001b[0;32m     29\u001b[0m grouped_reviews \u001b[38;5;241m=\u001b[39m df_with_predictions[df_with_predictions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Rating\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m'\u001b[39m] \\\n\u001b[0;32m     30\u001b[0m                   \u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShow\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin)\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predicted_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Instantiating the ReviewAnalysisPipeline with the scikit-learn pipeline\n",
    "review_analysis_pipeline = ReviewAnalysisPipeline(pipeline, encoder, role_focus='studio_production', num_sentences=3)\n",
    "\n",
    "# Processing reviews to filter and group high-rating ones\n",
    "grouped_reviews = review_analysis_pipeline.process_reviews(df, X_test)\n",
    "\n",
    "# Summarizing the grouped reviews\n",
    "final_summaries = review_analysis_pipeline.summarize_reviews(grouped_reviews)\n",
    "print(final_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4de8b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming analysis_pipeline is an instance of ReviewAnalysisPipeline\n",
    "grouped_reviews, df_with_predictions = analysis_pipeline.process_reviews(df, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a714d2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m analysis_pipeline \u001b[38;5;241m=\u001b[39m ReviewAnalysisPipeline(pipeline, encoder)\n\u001b[1;32m----> 2\u001b[0m df_with_predictions \u001b[38;5;241m=\u001b[39m \u001b[43manalysis_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Ensure process_reviews was called to set predicted_df\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(analysis_pipeline\u001b[38;5;241m.\u001b[39mpredicted_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "Input \u001b[1;32mIn [59]\u001b[0m, in \u001b[0;36mReviewAnalysisPipeline.process_reviews\u001b[1;34m(self, df, X_test)\u001b[0m\n\u001b[0;32m     24\u001b[0m predicted_categories \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39minverse_transform(predicted_categories) \n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicted_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(predicted_categories, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Rating\u001b[39m\u001b[38;5;124m'\u001b[39m], index\u001b[38;5;241m=\u001b[39mX_test\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m---> 26\u001b[0m df_with_predictions \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mpredicted_df\u001b[49m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Existing logic to filter and group reviews\u001b[39;00m\n\u001b[0;32m     29\u001b[0m grouped_reviews \u001b[38;5;241m=\u001b[39m df_with_predictions[df_with_predictions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Rating\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m'\u001b[39m] \\\n\u001b[0;32m     30\u001b[0m                   \u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShow\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin)\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predicted_df' is not defined"
     ]
    }
   ],
   "source": [
    "analysis_pipeline = ReviewAnalysisPipeline(pipeline, encoder)\n",
    "df_with_predictions = analysis_pipeline.process_reviews(df, X_test)\n",
    "# Ensure process_reviews was called to set predicted_df\n",
    "print(analysis_pipeline.predicted_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6b933024",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'simple_summarize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming simple_summarize is defined and ready to use\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m grouped_reviews[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeneral_Summary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m grouped_reviews[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[43msimple_summarize\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'simple_summarize' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming simple_summarize is defined and ready to use\n",
    "grouped_reviews['General_Summary'] = grouped_reviews['Review'].apply(simple_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ae5ad388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show: Alice in Borderland\n",
      "\n",
      "General Summary:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'General_Summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3361\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3362\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:108\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'General_Summary'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [71]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShow: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShow\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeneral Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGeneral_Summary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Assuming the general summary is stored in this column\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# If you have a studio production-focused summary or any other specific summary, print it as well\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStudio_production_Summary\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m grouped_reviews\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:942\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1051\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1048\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1051\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3363\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3362\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3363\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scalar(key) \u001b[38;5;129;01mand\u001b[39;00m isna(key) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhasnans:\n\u001b[0;32m   3366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'General_Summary'"
     ]
    }
   ],
   "source": [
    "# Iterate through each row in the DataFrame to print the show name and its summaries\n",
    "for index, row in grouped_reviews.iterrows():\n",
    "    print(f\"Show: {row['Show']}\\n\")\n",
    "    print(\"General Summary:\")\n",
    "    print(row['General_Summary'], \"\\n\")  # Assuming the general summary is stored in this column\n",
    "    \n",
    "    # If you have a studio production-focused summary or any other specific summary, print it as well\n",
    "    if 'Studio_production_Summary' in grouped_reviews.columns:\n",
    "        print(\"Studio Production Summary:\")\n",
    "        print(row['Studio_production_Summary'], \"\\n\")\n",
    "    \n",
    "    print(\"------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "de925400",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'simple_summarize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming 'simple_summarize' is your summarization function\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Ensure it's defined to accept a string (review text) and returns a summary string\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Apply the summarization function to each row in the 'Review' column\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m grouped_reviews[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeneral_Summary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m grouped_reviews[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[43msimple_summarize\u001b[49m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# After this operation, check if the column exists\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(grouped_reviews\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'simple_summarize' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming 'simple_summarize' is your summarization function\n",
    "# Ensure it's defined to accept a string (review text) and returns a summary string\n",
    "\n",
    "# Apply the summarization function to each row in the 'Review' column\n",
    "grouped_reviews['General_Summary'] = grouped_reviews['Review'].apply(simple_summarize)\n",
    "\n",
    "# After this operation, check if the column exists\n",
    "print(grouped_reviews.columns)  # This should now include 'General_Summary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8dc17e1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "simple_summarize() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [74]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming 'analysis_pipeline' is an instance of 'ReviewAnalysisPipeline'\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m final_grouped_reviews \u001b[38;5;241m=\u001b[39m \u001b[43manalysis_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummarize_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrouped_reviews\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [59]\u001b[0m, in \u001b[0;36mReviewAnalysisPipeline.summarize_reviews\u001b[1;34m(self, grouped_reviews)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize_reviews\u001b[39m(\u001b[38;5;28mself\u001b[39m, grouped_reviews):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Generate general and role-focused summaries for each group of reviews\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     grouped_reviews[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeneral_Summary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgrouped_reviews\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimple_summarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_sentences\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     grouped_reviews[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrole_focus\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Summary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m grouped_reviews[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m text: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_role_based_summary(text, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrole_focus, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_sentences))\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grouped_reviews\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4357\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4248\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4249\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4252\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4253\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FrameOrSeriesUnion:\n\u001b[0;32m   4254\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4255\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4256\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4355\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4356\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1043\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1098\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1092\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1093\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;66;03m# GH 25959 use pd.array instead of tolist\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m     \u001b[38;5;66;03m# so extension arrays can be used\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(pd_array(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2859\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [59]\u001b[0m, in \u001b[0;36mReviewAnalysisPipeline.summarize_reviews.<locals>.<lambda>\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarize_reviews\u001b[39m(\u001b[38;5;28mself\u001b[39m, grouped_reviews):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Generate general and role-focused summaries for each group of reviews\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     grouped_reviews[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeneral_Summary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m grouped_reviews[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m---> 38\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m text: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimple_summarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_sentences\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     39\u001b[0m     grouped_reviews[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrole_focus\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Summary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m grouped_reviews[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     40\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m text: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_role_based_summary(text, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrole_focus, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_sentences))\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grouped_reviews\n",
      "\u001b[1;31mTypeError\u001b[0m: simple_summarize() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# Assuming 'analysis_pipeline' is an instance of 'ReviewAnalysisPipeline'\n",
    "final_grouped_reviews = analysis_pipeline.summarize_reviews(grouped_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ae5cb89d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [79]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m grouped_reviews, _ \u001b[38;5;241m=\u001b[39m \u001b[43manalysis_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Process reviews to get grouped_reviews\u001b[39;00m\n\u001b[0;32m      2\u001b[0m final_grouped_reviews \u001b[38;5;241m=\u001b[39m analysis_pipeline\u001b[38;5;241m.\u001b[39msummarize_reviews(grouped_reviews)\n",
      "Input \u001b[1;32mIn [59]\u001b[0m, in \u001b[0;36mReviewAnalysisPipeline.process_reviews\u001b[1;34m(self, df, X_test)\u001b[0m\n\u001b[0;32m     24\u001b[0m predicted_categories \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39minverse_transform(predicted_categories) \n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicted_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(predicted_categories, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Rating\u001b[39m\u001b[38;5;124m'\u001b[39m], index\u001b[38;5;241m=\u001b[39mX_test\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m---> 26\u001b[0m df_with_predictions \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mpredicted_df\u001b[49m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Existing logic to filter and group reviews\u001b[39;00m\n\u001b[0;32m     29\u001b[0m grouped_reviews \u001b[38;5;241m=\u001b[39m df_with_predictions[df_with_predictions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Rating\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m'\u001b[39m] \\\n\u001b[0;32m     30\u001b[0m                   \u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShow\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin)\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predicted_df' is not defined"
     ]
    }
   ],
   "source": [
    "grouped_reviews, _ = analysis_pipeline.process_reviews(df, X_test)  # Process reviews to get grouped_reviews\n",
    "final_grouped_reviews = analysis_pipeline.summarize_reviews(grouped_reviews)  # Apply summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7e9ab4ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_grouped_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [76]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfinal_grouped_reviews\u001b[49m\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShow: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShow\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeneral Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final_grouped_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "for index, row in final_grouped_reviews.iterrows():\n",
    "    print(f\"Show: {row['Show']}\\n\")\n",
    "    print(\"General Summary:\")\n",
    "    print(row['General_Summary'], \"\\n\")  # Assuming the general summary is stored in this column\n",
    "    \n",
    "    # If you have a specific summary such as studio production, print it as well\n",
    "    if 'Studio_production_Summary' in final_grouped_reviews.columns:\n",
    "        print(\"Studio Production Summary:\")\n",
    "        print(row['Studio_production_Summary'], \"\\n\")\n",
    "    \n",
    "    print(\"-\" * 75)  # Separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da5b5df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
